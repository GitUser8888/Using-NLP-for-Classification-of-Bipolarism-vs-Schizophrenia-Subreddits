{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Background\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is done as part of the Data Science Immersive course in General Assembly, and was completed in a short time frame of only 2 weeks. We used Natural Language Processing (NLP) to build a model that classifies if a reddit post belongs to r/schizophrenia or r/bipolar.\n",
    "\n",
    "### Context\n",
    "To diagnose and assess mental health conditions, clinicians rely on:\n",
    "\n",
    "**F2F interactions**\n",
    "- Non-verbal cues like body and affect\n",
    "- Other possible associated symptoms\n",
    "- Physical state of the patient\n",
    "- Family members to ask\n",
    "\n",
    "**Direct diagnostic questioning**\n",
    "- Questions that get straight to the point\n",
    "- \"How is your mood lately?\"\n",
    "- \"Do you ever feel like your thoughts can be heard?\"\n",
    "\n",
    "### Challenges from the Emergence and Rise of Telemedicine\n",
    "Increased accessibility and convenience, but at the cost of:\n",
    "- Lack of non-verbal cues\n",
    "- Limited rapport building\n",
    "- Potential for misdiagnosis\n",
    "\n",
    "Current diagnostic methods are also not perfect. Majority of mental health patients will remain undiagnosed, at about 50% of all schizophrenia patients, and about 70% of all bipolar patients\n",
    "The primary healthcare system would hence greatly benefit from a preliminary alert filtering tool that is anonymous and community based.\n",
    "\n",
    "**There is a need for better linguistic understanding of mental health conditions.**\n",
    "\n",
    "### Objectives and Scope of this Project\n",
    "- Evaluate effectiveness of Natural-Language Processing (NLP) technologies as a way of converting and understanding mental health conditions.\n",
    "- Build a model that will assist clinicians in understanding how mental health can be diagnosed via non-contextual linguistic information\n",
    "- Test the model on text data taken from online communities r/bipolar and r/schizophrenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scraping Schizophrenia and Bipolar Subreddits using PRAW\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next section we will use praw to scrape r/schizophrenia and r/bipolar subreddits.\n",
    "\n",
    "For each subreddit, we will scrape the posts via each subreddit post display option, namely:\n",
    "1. Hot posts\n",
    "2. Controversial posts\n",
    "3. New posts\n",
    "4. Rising posts\n",
    "5. Top posts\n",
    "\n",
    "For each posts, we will scrape the following attributes:\n",
    "1. id\n",
    "2. title\n",
    "3. text\n",
    "4. post score\n",
    "5. number of comments\n",
    "6. author\n",
    "7. created time in utc\n",
    "8. gilded number (number of awards)\n",
    "\n",
    "Finally, the posts will be put into a dataframe and then exported to csv for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id ='zWndV5hUJ8XZYshw1V9axw',\n",
    "                     client_secret ='4ovj54M6J9s4yS6alG7c58jKOa3TQA',\n",
    "                     user_agent ='my user agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = ['bipolar', 'schizophrenia']\n",
    "submissions_types = ['hot', 'controversial', 'new', 'rising', 'top']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape for bipolar subreddit\n",
      "\n",
      "Hot posts from bipolar subreddit scraped\n",
      "Controversial posts from bipolar subreddit scraped\n",
      "New posts from bipolar subreddit scraped\n",
      "Rising posts from bipolar subreddit scraped\n",
      "Top posts from bipolar subreddit scraped\n",
      "Scraping for bipolar subreddit completed successfully\n",
      "Exporting to csv for bipolar subreddit completed successfully\n",
      "----------------------------------------\n",
      "Starting to scrape for schizophrenia subreddit\n",
      "\n",
      "Hot posts from schizophrenia subreddit scraped\n",
      "Controversial posts from schizophrenia subreddit scraped\n",
      "New posts from schizophrenia subreddit scraped\n",
      "Rising posts from schizophrenia subreddit scraped\n",
      "Top posts from schizophrenia subreddit scraped\n",
      "Scraping for schizophrenia subreddit completed successfully\n",
      "Exporting to csv for schizophrenia subreddit completed successfully\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for x in subs:\n",
    "    posts = []\n",
    "    subreddit = reddit.subreddit(x)\n",
    "\n",
    "    print(f'Starting to scrape for {x} subreddit')\n",
    "    print()\n",
    "\n",
    "    for submission_type in submissions_types:\n",
    "        submission_generator = getattr(subreddit, submission_type)(limit=1000)\n",
    "        posts.extend([[submission.id, submission.title, submission.selftext, submission.score, submission.num_comments, submission.author, submission.created_utc, submission.gilded] for submission in submission_generator])\n",
    "        print(f'{submission_type.capitalize()} posts from {x} subreddit scraped')\n",
    "\n",
    "    print(f'Scraping for {x} subreddit completed successfully')\n",
    "\n",
    "    df = pd.DataFrame(posts, columns=['id', 'title', 'text', 'score', 'comments_count', 'author', 'created_utc', 'gilding'])\n",
    "    df.to_csv(f'{x}.csv', index = False)\n",
    "\n",
    "    print(f'Exporting to csv for {x} subreddit completed successfully')\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using .json method to scrape r/schizophrenia and r/bipolar\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = ['https://www.reddit.com/r/bipolar.json', 'https://www.reddit.com/r/schizophrenia.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape for bipolar subreddit\n",
      "Sleeping for 6 seconds.\n",
      "Sleeping for 3 seconds.\n",
      "Sleeping for 6 seconds.\n",
      "Sleeping for 2 seconds.\n",
      "Scraping and exporting to csv for bipolar subreddit completed successfully\n",
      "------------------------------\n",
      "Starting to scrape for schizophrenia subreddit\n",
      "Sleeping for 4 seconds.\n",
      "Sleeping for 5 seconds.\n",
      "Sleeping for 4 seconds.\n",
      "Sleeping for 5 seconds.\n",
      "Scraping and exporting to csv for schizophrenia subreddit completed successfully\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for url in url_list:\n",
    "    headers = {'User-agent': 'Pony Inc 1.0'}\n",
    "    posts = []\n",
    "    after = None\n",
    "    \n",
    "    print(f'Starting to scrape for {subs[sub_count]} subreddit')\n",
    "\n",
    "    for a in range(4):\n",
    "        if after:\n",
    "            current_url = f\"{url}?after={after}\"\n",
    "        else:\n",
    "            current_url = url\n",
    "\n",
    "        res = requests.get(current_url, headers=headers)\n",
    "        \n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code)\n",
    "            break\n",
    "        \n",
    "        current_dict = res.json()\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "        \n",
    "        if a > 0:\n",
    "            prev_posts = pd.read_csv(f'ryan_code_{subs[sub_count]}.csv')\n",
    "            current_df = pd.DataFrame(posts)\n",
    "            combined_df = pd.concat([prev_posts, current_df])\n",
    "            combined_df.to_csv(f'ryan_code_{subs[sub_count]}.csv', index = False)\n",
    "        else:\n",
    "            pd.DataFrame(posts).to_csv(f'ryan_code_{subs[sub_count]}.csv', index = False)\n",
    "\n",
    "        # generate a random sleep duration to look more 'natural'\n",
    "        sleep_duration = random.randint(2,6)\n",
    "        print('Sleeping for', sleep_duration, 'seconds.')\n",
    "        time.sleep(sleep_duration)\n",
    "    \n",
    "    print(f'Scraping and exporting to csv for {subs[sub_count]} subreddit completed successfully')\n",
    "    print('------------------------------')\n",
    "    sub_count +=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
